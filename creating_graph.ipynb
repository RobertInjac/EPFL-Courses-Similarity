{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an EPFL Courses Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook similarity_method, we established and evaluated the method for calculating similarity/relatedness between two EPFL courses. In this notebook, __we will create a graph of EPFL courses.__ We want to create a graph in such a way that two related courses share a link. \n",
    "\n",
    "The notebook is divided into two parts:\n",
    "1. __Pre-processing:__ Data cleaning, fixing inconsistencies, pre-processing pipeline, creating final dataframe. \n",
    "2. __Graph-building:__ Using similarity method on pre-processed course data to build the graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rinjac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rinjac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import nltk, re, itertools, getpass, pymysql.cursors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "#sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "#nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing is the similar as in the notebook similarity_method.\n",
    "\n",
    "Firstly, let's get the datasets. We have three datasets, which are:\n",
    "1. __course_descriptions__ - Contains course name, description, summary, and other data for __all EPFL courses__\n",
    "2. __course_keywords__ - Contains keywords for each EPFL course (Keywords are usually main concepts taught in the course)\n",
    "3. __course_semesters__ - Contains in which semester is each course mostly taken (for example, Master 2. semester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/course_desc.csv', index_col=0, keep_default_na=False)\n",
    "kwdf = pd.read_csv('datasets/course_keywords.csv', index_col=0, keep_default_na=False)\n",
    "smdf = pd.read_csv('datasets/course_semesters.csv', index_col=0, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing minor inconsistencies in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a lof of database rows, we don't have the course title in English ('CourseNameEN')\n",
    "# but the 'CourseNameFR' field is actually title in English\n",
    "# so we fix this\n",
    "df['CourseName'] = df.apply(lambda row: row['CourseNameFR'] if not row['CourseNameEN'] else row['CourseNameEN'], axis=1)\n",
    "df = df.drop(columns=['CourseNameEN', 'CourseNameFR'])\n",
    "df['CourseName'] = df['CourseName'].apply(lambda x: \" \" if not x else x)\n",
    "\n",
    "# concat the course content and summary\n",
    "df['CourseContent'] = df['CourseContent'] + df['SummaryEN']\n",
    "df = df.drop(columns=['SummaryEN'])\n",
    "\n",
    "# remove rows which still have null values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding keywords to main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group keywords by course code\n",
    "kwdf = kwdf.groupby('CourseCode')['TagValue'].agg(lambda col: ' '.join(col))\n",
    "kwdf = pd.DataFrame(kwdf)\n",
    "\n",
    "# add keywords for DF dataframe\n",
    "df = df.set_index(keys=['CourseCode'], drop=True)\n",
    "df = df.merge(kwdf, on=['CourseCode'])\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining pre-processing functions we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonascii(s):\n",
    "    return s.encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "\n",
    "def remove_newline(s):\n",
    "    return s.replace('\\n','')\n",
    "\n",
    "def remove_squote(s):\n",
    "    return s.replace('<squote/>',' ')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stop_words(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "french_stop_words = stopwords.words('french')\n",
    "def remove_french_stop_words(tokens):\n",
    "    return [word for word in tokens if word not in french_stop_words]\n",
    "\n",
    "punc = list(string.punctuation)\n",
    "def remove_punc(tokens):\n",
    "    return [word for word in tokens if word not in punc]\n",
    "\n",
    "def to_lower(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def apply_preproc(df, column, func): \n",
    "    df[column] = df[column].apply(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Finding most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For reasons explained in notebook similarity_method (in the section 2.2)__ we will write the code which finds the N most common words in course descriptions and a pre-processing function which will remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['course', 'semicolon/', 'students', 'design', 'systems', 'analysis', 'methods', 'introduction', 'squote/', 'basic', 'models', 'techniques', 'energy', 'project', 'theory', 'concepts', '2', 'applications', 'materials', 'data', 'research', '3', 'different', 'processes', 'principles', 'system', 'tools', 'student', 'development', 'work', 'well', 'engineering', 'field', 'linear', 'topics', 'management', 'properties', 'part', 'also', 'control', '4', 'use', 'main', 'understanding', 'practical', 'learning', 'social', 'knowledge', 'using', 'study', 'understand', 'modeling', 'theoretical', 'problems', 'examples', 'various', 'structure', 'aspects', 'digital', 'science', '5', 'process', 'urban', 'fundamental', 'based', '1', 'application', 'new', \"''\", 'specific', 'used', 'processing', 'issues', 'information', 'time', 'approach', 'power', 'model', 'structures', 'learn', 'physical', 'chemical', '``', 'studies', 'case', 'technology', 'related', 'general', 'types', 'optical', 'quantum', 'physics', 'including', 'equations', 'environmental', 'class', '6', 'algorithms', 'modern', 'architecture']\n"
     ]
    }
   ],
   "source": [
    "ndf = df.copy()\n",
    "\n",
    "str_list = []\n",
    "for i in range(len(df)):\n",
    "    str_list.append(df.iloc[i]['CourseContent'])\n",
    "all_content = ''.join(str_list)\n",
    "                    \n",
    "all_content = word_tokenize(all_content)\n",
    "all_content = to_lower(all_content)\n",
    "all_content = remove_stop_words(all_content)\n",
    "all_content = remove_punc(all_content)\n",
    "\n",
    "freq_d = dict()\n",
    "for w in all_content:\n",
    "    freq_d[w] = 1 + freq_d.get(w, 0)\n",
    "    \n",
    "freq = [(freq_d[key], key) for key in freq_d]\n",
    "freq.sort()\n",
    "freq.reverse()\n",
    "\n",
    "most_common_f = freq[:100]\n",
    "_, most_common = [list(tup) for tup in zip(*most_common_f)]\n",
    "print(most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_most_common_words(tokens):\n",
    "    return [word for word in tokens if word not in most_common]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating final dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to use pre-processing functions to create the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preproc(df, column, func): \n",
    "    df[column] = df[column].apply(func)\n",
    "    \n",
    "def keep_unique_courses(df):\n",
    "    df['CourseCode'] = df['CourseCode'].apply(lambda x: x.split('(')[0])\n",
    "    df = df.drop_duplicates(subset=['CourseCode'], keep='first')\n",
    "    return df\n",
    "\n",
    "# we need course name for graph visualizaion\n",
    "df['CourseName2'] = df['CourseName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep unique courses (some courses have variants (a), (b), (c))\n",
    "df = keep_unique_courses(df)\n",
    "\n",
    "columns = ['CourseContent', 'CourseName']\n",
    "\n",
    "for column in columns:\n",
    "    # tokenization\n",
    "    apply_preproc(df, column, word_tokenize)\n",
    "    # to lower\n",
    "    apply_preproc(df, column, to_lower)\n",
    "    # remove stop words\n",
    "    apply_preproc(df, column, remove_stop_words)\n",
    "    # remove french stop words\n",
    "    apply_preproc(df, column, remove_french_stop_words)\n",
    "    # remove punc\n",
    "    apply_preproc(df, column, remove_punc)\n",
    "\n",
    "# remove most common words\n",
    "# ONLY for the course content\n",
    "apply_preproc(df, 'CourseContent', remove_most_common_words)\n",
    "\n",
    "# merge name and content\n",
    "df['CourseContent'] = df['CourseName'] + df['CourseContent']\n",
    "df = df[['CourseCode', 'CourseContent', 'CourseName2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to pre-process the semesters dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smdf = smdf.sort_values('NRegistrations', ascending=False).drop_duplicates(['CourseCode'])\n",
    "smdf = smdf[['CourseCode', 'PedagogicalName']]\n",
    "smdf = smdf.dropna()\n",
    "\n",
    "# keep unique courses (some courses have variants (a), (b), (c))\n",
    "smdf = keep_unique_courses(smdf)\n",
    "\n",
    "smdf = smdf.set_index(['CourseCode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the semester names into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pedname(pedname):\n",
    "    if pedname == 'Bachelor semestre 1':\n",
    "        return 1\n",
    "    elif pedname == 'Bachelor semestre 2':\n",
    "        return 2\n",
    "    elif pedname == 'Bachelor semestre 3':\n",
    "        return 3\n",
    "    elif pedname == 'Bachelor semestre 4':\n",
    "        return 4\n",
    "    elif pedname == 'Bachelor semestre 5':\n",
    "        return 5\n",
    "    elif pedname == 'Bachelor semestre 5b':\n",
    "        return 5\n",
    "    elif pedname == 'Bachelor semestre 6':\n",
    "        return 6\n",
    "    elif pedname == 'Bachelor semestre 6b':\n",
    "        return 6\n",
    "    elif pedname == 'Master semestre 1':\n",
    "        return 7\n",
    "    elif pedname == 'Master semestre 2':\n",
    "        return 8\n",
    "    elif pedname == 'Master semestre 3':\n",
    "        return 9\n",
    "    elif pedname == 'Master semestre 4':\n",
    "        return 10\n",
    "    elif pedname == 'Projet master automne':\n",
    "        return 9\n",
    "    elif pedname == 'Projet master printemps':\n",
    "        return 10\n",
    "    elif pedname == 'Semestre automne':\n",
    "        return 9\n",
    "    elif pedname == 'Semestre printemps':\n",
    "        return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "smdf['PedagogicalName'] = smdf['PedagogicalName'].apply(lambda x: convert_pedname(x))\n",
    "sem_dict = smdf.to_dict('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph-building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will __build the final graph of EPFL courses.__ \n",
    "\n",
    "The graph will have the following structure:\n",
    "* __Nodes:__ EPFL courses\n",
    "* __Edges:__ Two courses will share an edge if they are related (according to our similarity method)\n",
    "\n",
    "The graph is going to be __directed__. We have the information which courses are taken when (course_semesters dataset), and we will __only conect courses from earlier to later.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained word2vec on wikipedia\n",
    "model = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')\n",
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for getting the average word embedding out of list of words\n",
    "def get_average_vector(words_list):\n",
    "    base = np.zeros(EMBEDDING_SIZE)\n",
    "    word_vec = 0\n",
    "    n = 0\n",
    "    for word in words_list:\n",
    "        try:\n",
    "            word_vec = model[word]\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            word_vec = np.zeros(300)\n",
    "        base = np.add(base, word_vec)\n",
    "    base = np.divide(base, n)\n",
    "    \n",
    "    return base.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a threshold. Based on experiences in similarity_method, we will pick a threshold of 0.88."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node\n",
    "subjectID = []\n",
    "subject_name = []\n",
    "course_codes = []\n",
    "\n",
    "# edge\n",
    "source = []\n",
    "target = []\n",
    "type_ = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    list1 = df.iloc[i]['CourseContent']\n",
    "    base1 = get_average_vector(list1)\n",
    "    code1 = df.iloc[i]['CourseCode']\n",
    "    \n",
    "    # make node\n",
    "    subjectID.append(str(i))\n",
    "    subject_name.append(df.iloc[i]['CourseName2'])\n",
    "    course_codes.append(code1)\n",
    "    \n",
    "    nodes = []\n",
    "    for j in range(i+1, len(df)):\n",
    "        code2 = df.iloc[j]['CourseCode']\n",
    "        list2 = df.iloc[j]['CourseContent']\n",
    "        base2 = get_average_vector(list2)\n",
    "\n",
    "        sim = cosine_similarity(base1, base2)\n",
    "        sim = sim[0][0]\n",
    "        \n",
    "        if sim < THRESHOLD:\n",
    "            continue\n",
    "        \n",
    "        # as said, we only connect from earlier course to latter\n",
    "        if sem_dict.get(code1) is not None and sem_dict.get(code2) is not None:\n",
    "            if sem_dict.get(code1)['PedagogicalName'] < sem_dict.get(code2)['PedagogicalName']:\n",
    "                nodes.append((j, sim, 'sd'))\n",
    "            elif sem_dict.get(code1)['PedagogicalName'] > sem_dict.get(code2)['PedagogicalName']:\n",
    "                nodes.append((j, sim, 'ds'))\n",
    "    \n",
    "    # we take top 3 most relevant courses for this course, and connect them to it\n",
    "    nodes = sorted(nodes, key=lambda x: x[1])\n",
    "    nodes = nodes[:3]\n",
    "    for node in nodes:\n",
    "        j = node[0]\n",
    "        \n",
    "        if node[2] == 'sd':\n",
    "            source.append(str(i))\n",
    "            target.append(str(j))\n",
    "        else:\n",
    "            source.append(str(j))\n",
    "            target.append(str(i))\n",
    "        type_.append(\"indicative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the graph's nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'source': source, 'target': target, 'type': type_}\n",
    "edges = pd.DataFrame(data=d)\n",
    "edges.to_csv('graph/edges.csv', index=False)\n",
    "\n",
    "d = {'id': subjectID, 'subjectname': subject_name, 'associatedcoursecodes': course_codes}\n",
    "nodes = pd.DataFrame(data=d)\n",
    "nodes.to_csv('graph/nodes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
